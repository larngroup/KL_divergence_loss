
# KL Divergence Annealing Schedule

Simple framework for implementation of the KL loss annealing schedule on any Variational Autoencoder (VAE) with an autoregressive decoder. 
Code is implemented with tensorflow and keras

# Background 
VAEs with an autoregressive decoder typically suffer from an issue named "KL vanishing problem", in which the decoder, during training, ignores the information in the latent space and relies solely on the information that was generated by it on the last time step. Broadly speaking, this happens because the latent space in the beginning of the training stage has very low information quality, so the decoder learns to rely mostly on its past information.

An improvement on the quality of latent space could potentially mitigate this issue, more specifically enhancing the training of the encoder model during training could provide a latent with more useful information. A typically VAE has its loss divided in a reconstruction loss, that measures the difference between a prediction and the corresponding ground truth, and kl divergence loss that measures the similarity between the distributions in the latent space and a unit gaussian distribution. A kl annealing approach would consist on giving the kl divergence loss a weight, Beta, typically starting with a low value (e.g. 0). The loss function could then be presented as in the next figure:

<p align="center">
  <img src="https://user-images.githubusercontent.com/24720785/141496165-5ad94e67-4619-4295-bafe-8ca768e79a32.png" />
</p>

Three main schedules for Beta exist:

- Constant Schedule: Maintaining a constant Beta would produce a typical loss of a VAE;
- Monotonical Annealing Schedule: A Beta parameter that increases during training;
- Cyclical Annealing Schedule: A Beta parameter that increases, and then resets back to low values, several times during training, in a cyclical manner. 

For more information on the subject the reader is redirected to the paper in Reference section. 


# Implementation Instructions

1 - Import the classes in the annealing_helper_objects.py:

<p align="center">
  <img src="https://user-images.githubusercontent.com/24720785/141499064-9db8d399-0d07-431b-bc5d-efb000474da5.png" />
</p>

2 - Outside the model building methods define the tensor representing the beta variable (beta_var):

<p align="center">
  <img src="https://user-images.githubusercontent.com/24720785/141499290-cd990ac0-d3e1-4fd9-95ed-0cdb8b6e01d7.png" />
</p>

3 - From the latent space distribution and beta variable generate (using the "Kl_annealing_loss" class) the loss and add the loss and its corresponding metric to the model:

<p align="center">
  <img src="https://user-images.githubusercontent.com/24720785/141500648-04be2dd4-2ff9-4df2-be14-9da1bff89087.png" />
</p>

4 - Initialize custom callback and metric classes and use them in the compilation and fit methods:

<p align="center">
  <img src="https://user-images.githubusercontent.com/24720785/141500985-f99ef9d7-c7c1-4cf5-ae2e-d7c0c34e6938.png" />
</p>


For implementation of custom annealing schedules directly change callback class "AnnealingCallback". 

# Results

As mentioned previously this framework was implemented in a simple VAE. Since the decoder is not autoregressive the potential benefits of annealing schedules will not be observable. It was implemented in a simple VAE for debugging purposes. It is expected in future work to implement this framework on autoencoders with autoregressive decoders (the intended beneficiaries).


## Constant Schedule

<p align="center" float="left">
  <img src="https://user-images.githubusercontent.com/24720785/141502473-9950a2e2-7c81-426a-bdde-7a93f7dc6618.png" width="400" />
  <img src="https://user-images.githubusercontent.com/24720785/141502545-b3598918-e1b9-4026-b266-2920d579a4f0.png" width="400" /> 
</p>

## Monotonic Annealing Schedule

<p align="center" float="left">
  <img src="https://user-images.githubusercontent.com/24720785/141502936-688e3d81-24c0-4333-96b3-bd70f2cf2947.png" width="400" />
  <img src="https://user-images.githubusercontent.com/24720785/141502991-ecf330d9-be46-4d5a-b253-fad9a354ccdb.png" width="400" /> 
</p>

## Cyclical Annealing Schedule

<p align="center" float="left">
  <img src="https://user-images.githubusercontent.com/24720785/141503118-fafcc6e1-b2d6-4c40-b772-c4c965db13e0.png" width="400" />
  <img src="https://user-images.githubusercontent.com/24720785/141503168-45633460-8be7-4270-8a04-9bb57ed2c845.png" width="400" /> 
</p>

# Reference

<a id="1">[1]</a>
Fu, H., Li, C., Liu, X., Gao, J., Ã‡elikyilmaz, A., & Carin, L. (2019). Cyclical Annealing Schedule: A Simple Approach to Mitigating KL Vanishing. NAACL.

