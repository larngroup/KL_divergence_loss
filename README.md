
# KL Divergence Annealing Schedule

Simple framework for implementation of the KL loss annealing schedule on any Variational Autoencoder (VAE) with an autoregressive decoder. 
Code is implemented with tensorflow and keras

# Background 
VAEs with an autoregressive decoder typically suffer from an issue named "KL vanishing problem", in which the decoder, during training, ignores the information in the latent space and relies solely on the information that was generated by it on the last time step. Broadly speaking, this happens because the latent space in the beginning of the training stage has very low information quality, so the decoder learns to rely mostly on its past information.

An improvement on the quality of latent space could potentially mitigate this issue, more specifically enhancing the training of the encoder model during training could provide a latent with more useful information. A typically VAE has its loss divided in a reconstruction loss, that measures the difference between a prediction and the corresponding ground truth, and kl divergence loss that measures the similarity between the distributions in the latent space and a unit gaussian distribution. A kl annealing approach would consist on giving the kl divergence loss a weight, Beta, typically starting with a low value (e.g. 0). The loss function could then be presented as in the next figure:

<p align="center">
  <img src="https://user-images.githubusercontent.com/24720785/141496165-5ad94e67-4619-4295-bafe-8ca768e79a32.png" />
</p>

Three main schedules for Beta exist:

- Constant Schedule: Maintaining a constant Beta would produce a typical loss of a VAE;
- Monotonical Annealing Schedule: A Beta parameter that increases during training;
- Cyclical Annealing Schedule: A Beta parameter that increases, and then resets back to low values, several times during training, in a cyclical manner. 

For more information on the subject the reader is redirected to the paper in Reference section. 


# Implementation Instructions

The code provided is specifically used for Variational Autoencoders. While its implementation is simple and easy to customize, some important details need to be addressed.

The class Annealing_model inherits all of tensorflow.keras.Model functions and essentially behaves the same way except for the added functionality.
Initializing the Annealing_model utilizes the same arguments as tensorflow.keras.Model and 2 more arguments as shown in the next example:

my_model=Annealing_model(annealing_mode,latent_distribution,*args,**kwargs)

annealing_mode: The mode in which the annealing process will occur. Available modes are "normal", "monotonic" and "cyclical";
latent_distribution: A tensor of the latent space vector (after the reparameterization trick);
*args,**kwargs: all the remaining arguments usually used for the tensorflow.keras.Model class.

The Annealing_model class prepares a reconstruction loss and its respective metric, so there is no need to include as an argument in the "compile" function. In this current version the reconstruction loss, and its respective metric, are hardcoded as a categorical cross-entropy loss. To implement a different loss, the user must change the loss defined in the function "compile" in the Annealing_model class. 

As mentioned there are currently only three modes of kl annealing. To add or change the currently implemented modes, change the function "on_epoch_end" of the class "AnnealingCallback"







# Reference

<a id="1">[1]</a>
Fu, H., Li, C., Liu, X., Gao, J., Ã‡elikyilmaz, A., & Carin, L. (2019). Cyclical Annealing Schedule: A Simple Approach to Mitigating KL Vanishing. NAACL.

